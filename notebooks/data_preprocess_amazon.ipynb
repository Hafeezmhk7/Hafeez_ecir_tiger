{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "import glob\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def parse_2023(path):\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"Parsing {path}\"):\n",
    "            yield json.loads(line)\n",
    "        \n",
    "'''\n",
    "Set seeds\n",
    "'''\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../dataset/amazon/2023/raw\"\n",
    "short_data_name = 'beauty'\n",
    "os.makedirs(os.path.join(DATASET_DIR, short_data_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if short_data_name == 'beauty':\n",
    "    full_data_name = 'Beauty'\n",
    "elif short_data_name == 'toys':\n",
    "    full_data_name = 'Toys_and_Games'\n",
    "elif short_data_name == 'sports':\n",
    "    full_data_name = 'Sports_and_Outdoors'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "DATA_NAME_MAP = {\n",
    "    'beauty': 'All_Beauty',\n",
    "    'toys': 'Toys_and_Games',\n",
    "    'sports': 'Sports_and_Outdoors'\n",
    "}\n",
    "\n",
    "INVERSE_DATA_NAME_MAP = {v: k for k, v in DATA_NAME_MAP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Sequential Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pickle_summary(data, title=\"Pickle File Contents\"):\n",
    "    \"\"\"\n",
    "    Load and summarize the contents of a pickle file using rich.\n",
    "\n",
    "    :param data: .pkl data\n",
    "    :param title: Optional title for the printed table.\n",
    "    \"\"\"\n",
    "    table = Table(title=title)\n",
    "    \n",
    "    table.add_column(\"Key/Type\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Description\", style=\"magenta\")\n",
    "\n",
    "    total_size = 0\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            desc = f\"{type(value).__name__}, len={len(value)}\" if hasattr(value, '__len__') else type(value).__name__\n",
    "            table.add_row(str(key), desc)\n",
    "            if key in [\"train\", \"test\", \"val\"]:\n",
    "                total_size += len(value)\n",
    "            else:\n",
    "                total_size = \"N/A\"\n",
    "    else:\n",
    "        table.add_row(type(data).__name__, f\"{data}\" if isinstance(data, (int, float, str)) else str(type(data)))\n",
    "\n",
    "    table.add_row(\"Total Size\", str(total_size))\n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "    \n",
    "    if \"train\" in data or \"test\" in data or \"val\" in data:\n",
    "        rprint(\"Train Sample:\")\n",
    "        rprint(data['train'][0])\n",
    "        rprint(\"Val Sample:\")\n",
    "        rprint(data['val'][0])\n",
    "        rprint(\"Test Sample:\")\n",
    "        print(data['test'][0])\n",
    "        \n",
    "        \n",
    "def df_stats(df: pd.DataFrame, title=\"DataFrame Stats\"):\n",
    "    table = Table(title=title)\n",
    "    rprint(f\"DataFrame shape: {df.shape}\")\n",
    "    table.add_column(\"Column\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Non-Null Count\", style=\"yellow\")\n",
    "    table.add_column(\"Unique Count\", style=\"magenta\")\n",
    "    table.add_column(\"Null/NA Count\", style=\"red\")\n",
    "    table.add_column(\"Data Type\", style=\"green\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            non_null_count = df[col].notna().sum()\n",
    "        except:\n",
    "            non_null_count = \"Error\"\n",
    "        try:\n",
    "            unique_count = df[col].nunique(dropna=True)\n",
    "        except:\n",
    "            unique_count = \"Error\"\n",
    "        try:\n",
    "            null_count = df[col].isna().sum()\n",
    "        except:\n",
    "            null_count = \"Error\"\n",
    "        try:\n",
    "            dtype = str(df[col].dtype)\n",
    "        except:\n",
    "            dtype = \"Error\"\n",
    "        table.add_row(col, str(non_null_count), str(unique_count), str(null_count), dtype)\n",
    "\n",
    "    Console().print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_amazon_dataset(category, metadata=False):\n",
    "    rprint(\"Reading Ratings...\")\n",
    "    raw_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "                       f\"raw_meta_{category}\" if metadata else f\"raw_review_{category}\", \n",
    "                       trust_remote_code=True)\n",
    "    raw_df = raw_dataset['full'].to_pandas()\n",
    "    # df_stats(raw_df, f\"{category} Reviews DataFrame Stats\")\n",
    "    \n",
    "    rprint(\"Reading Ratings...\")\n",
    "    rating_file = hf_hub_download(\n",
    "            repo_id='McAuley-Lab/Amazon-Reviews-2023',\n",
    "            filename=f'benchmark/5core/rating_only/{category}.csv',\n",
    "            repo_type='dataset'\n",
    "            )\n",
    "    rating_df = pd.read_csv(rating_file)\n",
    "    # df_stats(rating_df, f\"{category} Rating DataFrame Stats\")\n",
    "    \n",
    "    # create sets for filtering\n",
    "    valid_users = set(rating_df['user_id'].unique())\n",
    "    valid_items = set(rating_df['parent_asin'].unique())\n",
    "    \n",
    "    rprint(\"Filtering Data...\")\n",
    "    # filter reviews where both user_id and parent_asin are in the 5-core subset\n",
    "    if metadata:\n",
    "        filtered_data = raw_df[raw_df['parent_asin'].isin(valid_items)]\n",
    "    else:\n",
    "        filtered_data = raw_df[\n",
    "            (raw_df['user_id'].isin(valid_users)) & \n",
    "            (raw_df['parent_asin'].isin(valid_items))\n",
    "        ]\n",
    "    \n",
    "    # df_stats(filtered_reviews, f\"{category} Filtered Reviews DataFrame Stats\")\n",
    "    # rprint(\"Filtered Dataset Shape:\", filtered_data.shape)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# return (user item timestamp) sort in get_interaction\n",
    "def get_reviews(dataset_name, rating_score, year=2023):\n",
    "    '''\n",
    "    reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "    asin - ID of the product, e.g. 0000013714\n",
    "    reviewerName - name of the reviewer\n",
    "    helpful - helpfulness rating of the review, e.g. 2/3\n",
    "    --\"helpful\": [2, 3],\n",
    "    reviewText - text of the review\n",
    "    --\"reviewText\": \"I bought this for my husband who plays the piano. ...\"\n",
    "    overall - rating of the product\n",
    "    --\"overall\": 5.0,\n",
    "    summary - summary of the review\n",
    "    --\"summary\": \"Heavenly Highway Hymns\",\n",
    "    unixReviewTime - time of the review (unix time)\n",
    "    --\"unixReviewTime\": 1252800000,\n",
    "    reviewTime - time of the review (raw)\n",
    "    --\"reviewTime\": \"09 13, 2009\"\n",
    "    '''\n",
    "    data = []\n",
    "    # remove those with less than a certain score\n",
    "    # older Amazon\n",
    "    if not year == 2023:\n",
    "        data_file = './raw_data/reviews_' + dataset_name + '.json.gz'\n",
    "        for review in parse(data_file):\n",
    "            if float(review['overall']) <= rating_score:\n",
    "                continue\n",
    "            user = review['reviewerID']\n",
    "            item = review['asin']\n",
    "            time = review['unixReviewTime']\n",
    "            data.append((user, item, int(time)))\n",
    "    # latest Amazon\n",
    "    else:\n",
    "        # slow to parse directly, load with HF\n",
    "        data_file = os.path.join(DATASET_DIR, INVERSE_DATA_NAME_MAP[dataset_name], \"reviews.json.gz\")\n",
    "        if not os.path.exists(data_file):\n",
    "            # raise FileExistsError(\"Save the K-Core file first!\")\n",
    "            rprint(f\"Saving the K-Core for `{dataset_name}` Reviews\")\n",
    "            # save the k-core first\n",
    "            filtered_reviews = filter_amazon_dataset(dataset_name)\n",
    "            filtered_reviews.to_json(data_file, \n",
    "                         orient='records', \n",
    "                         lines=True,\n",
    "                         compression='gzip')\n",
    "        for review in parse_2023(data_file):\n",
    "            if float(review['rating']) <= rating_score:\n",
    "                continue\n",
    "            user = review['user_id']\n",
    "            item = review['parent_asin']\n",
    "            time = review['timestamp']\n",
    "            data.append((user, item, int(time)))\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_metadata(dataset_name, data_maps, year=2023):\n",
    "    '''\n",
    "    asin - ID of the product, e.g. 0000031852\n",
    "    --\"asin\": \"0000031852\",\n",
    "    title - name of the product\n",
    "    --\"title\": \"Girls Ballet Tutu Zebra Hot Pink\",\n",
    "    description\n",
    "    price - price in US dollars (at time of crawl)\n",
    "    --\"price\": 3.17,\n",
    "    imUrl - url of the product image (str)\n",
    "    --\"imUrl\": \"http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg\",\n",
    "    related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "    --\"related\":{\n",
    "        \"also_bought\": [\"B00JHONN1S\"],\n",
    "        \"also_viewed\": [\"B002BZX8Z6\"],\n",
    "        \"bought_together\": [\"B002BZX8Z6\"]\n",
    "    },\n",
    "    salesRank - sales rank information\n",
    "    --\"salesRank\": {\"Toys & Games\": 211836}\n",
    "    brand - brand name\n",
    "    --\"brand\": \"Coxlures\",\n",
    "    categories - list of categories the product belongs to\n",
    "    --\"categories\": [[\"Sports & Outdoors\", \"Other Sports\", \"Dance\"]]\n",
    "    '''\n",
    "    data = {}\n",
    "    item_asins = list(data_maps['item2id'].keys())\n",
    "\n",
    "    # older Amazon\n",
    "    if not year == 2023:\n",
    "        meta_file = './raw_data/meta_' + dataset_name + '.json.gz'\n",
    "        for info in parse(meta_file):\n",
    "            if info['asin'] not in item_asins:\n",
    "                continue\n",
    "            data[info['asin']] = info\n",
    "    # latest Amazon\n",
    "    else:\n",
    "        # slow to parse directly, load with HF\n",
    "        meta_file = os.path.join(DATASET_DIR, INVERSE_DATA_NAME_MAP[dataset_name], \"meta.json.gz\")\n",
    "        if not os.path.exists(meta_file):\n",
    "            # raise FileExistsError(\"Save the K-Core file first!\")\n",
    "            rprint(f\"Saving the K-Core for `{dataset_name}` Metadata\")\n",
    "            # save the k-core first\n",
    "            filtered_meta = filter_amazon_dataset(dataset_name, metadata=True)\n",
    "            filtered_meta.to_json(meta_file, \n",
    "                         orient='records', \n",
    "                         lines=True,\n",
    "                         compression='gzip')\n",
    "        for info in parse_2023(meta_file):\n",
    "            # comparsion not required as filtered data is already correct\n",
    "            # also, super slow parsing! need a better approach\n",
    "            # if info['parent_asin'] not in item_asins:\n",
    "            #     continue\n",
    "            data[info['parent_asin']] = info\n",
    "    return data\n",
    "\n",
    "def add_comma(num):\n",
    "    # 1000000 -> 1,000,000\n",
    "    str_num = str(num)\n",
    "    res_num = ''\n",
    "    for i in range(len(str_num)):\n",
    "        res_num += str_num[i]\n",
    "        if (len(str_num)-i-1) % 3 == 0:\n",
    "            res_num += ','\n",
    "    return res_num[:-1]\n",
    "\n",
    "# categories 和 brand is all attribute\n",
    "def get_attribute_amazon(meta_infos, datamaps, attribute_core, year=2023):\n",
    "    attributes = defaultdict(int)\n",
    "    for iid, info in tqdm(meta_infos.items()):\n",
    "        for cates in info['categories']:\n",
    "            for cate in cates[1:]: # 把主类删除 没有用\n",
    "                attributes[cate] +=1\n",
    "        if year == 2023:\n",
    "            brand = eval(info[\"details\"]).get(\"Brand\", \"Unknown\")\n",
    "            attributes[brand] += 1\n",
    "        else:\n",
    "            attributes[info['brand']] += 1\n",
    "\n",
    "    rprint(f'Before Delete, Attribute Num:{len(attributes)}')\n",
    "    new_meta = {}\n",
    "    for iid, info in tqdm(meta_infos.items()):\n",
    "        new_meta[iid] = []\n",
    "\n",
    "        if year == 2023:\n",
    "            brand = eval(info[\"details\"]).get(\"Brand\", \"Unknown\")\n",
    "            if attributes[brand] >= attribute_core:\n",
    "                new_meta[iid].append(brand)\n",
    "        else:\n",
    "            if attributes[info['brand']] >= attribute_core:\n",
    "                new_meta[iid].append(info['brand'])\n",
    "                \n",
    "        for cates in info['categories']:\n",
    "            for cate in cates[1:]:\n",
    "                if attributes[cate] >= attribute_core:\n",
    "                    new_meta[iid].append(cate)\n",
    "    # 做映射\n",
    "    attribute2id = {}\n",
    "    id2attribute = {}\n",
    "    attributeid2num = defaultdict(int)\n",
    "    attribute_id = 1\n",
    "    items2attributes = {}\n",
    "    attribute_lens = []\n",
    "\n",
    "    for iid, attributes in new_meta.items():\n",
    "        item_id = datamaps['item2id'][iid]\n",
    "        items2attributes[item_id] = []\n",
    "        for attribute in attributes:\n",
    "            if attribute not in attribute2id:\n",
    "                attribute2id[attribute] = attribute_id\n",
    "                id2attribute[attribute_id] = attribute\n",
    "                attribute_id += 1\n",
    "            attributeid2num[attribute2id[attribute]] += 1\n",
    "            items2attributes[item_id].append(attribute2id[attribute])\n",
    "        attribute_lens.append(len(items2attributes[item_id]))\n",
    "    rprint(f'After delete, Attribute Num:{len(attribute2id)}')\n",
    "    rprint(f'Attributes len, Min:{np.min(attribute_lens)}, Max:{np.max(attribute_lens)}, Avg.:{np.mean(attribute_lens):.4f}')\n",
    "    # 更新datamap\n",
    "    datamaps['attribute2id'] = attribute2id\n",
    "    datamaps['id2attribute'] = id2attribute\n",
    "    datamaps['attributeid2num'] = attributeid2num\n",
    "    return len(attribute2id), np.mean(attribute_lens), datamaps, items2attributes\n",
    "\n",
    "\n",
    "def get_interaction(datas):\n",
    "    user_seq = {}\n",
    "    for data in datas:\n",
    "        user, item, time = data\n",
    "        if user in user_seq:\n",
    "            user_seq[user].append((item, time))\n",
    "        else:\n",
    "            user_seq[user] = []\n",
    "            user_seq[user].append((item, time))\n",
    "\n",
    "    for user, item_time in user_seq.items():\n",
    "        item_time.sort(key=lambda x: x[1])  # 对各个数据集得单独排序\n",
    "        items = []\n",
    "        for t in item_time:\n",
    "            items.append(t[0])\n",
    "        user_seq[user] = items\n",
    "    return user_seq\n",
    "\n",
    "# K-core user_core item_core\n",
    "def check_Kcore(user_items, user_core, item_core):\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            user_count[user] += 1\n",
    "            item_count[item] += 1\n",
    "\n",
    "    for user, num in user_count.items():\n",
    "        if num < user_core:\n",
    "            return user_count, item_count, False\n",
    "    for item, num in item_count.items():\n",
    "        if num < item_core:\n",
    "            return user_count, item_count, False\n",
    "    return user_count, item_count, True # 已经保证Kcore\n",
    "\n",
    "# 循环过滤 K-core\n",
    "def filter_Kcore(user_items, user_core, item_core): # user 接所有items\n",
    "    user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    while not isKcore:\n",
    "        for user, num in user_count.items():\n",
    "            if user_count[user] < user_core: # 直接把user 删除\n",
    "                user_items.pop(user)\n",
    "            else:\n",
    "                for item in user_items[user]:\n",
    "                    if item_count[item] < item_core:\n",
    "                        user_items[user].remove(item)\n",
    "        user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    return user_items\n",
    "\n",
    "def id_map(user_items): # user_items dict\n",
    "    user2id = {} # raw 2 uid\n",
    "    item2id = {} # raw 2 iid\n",
    "    id2user = {} # uid 2 raw\n",
    "    id2item = {} # iid 2 raw\n",
    "    user_id = 1\n",
    "    item_id = 1\n",
    "    final_data = {}\n",
    "    random_user_list = list(user_items.keys())\n",
    "    random.shuffle(random_user_list)\n",
    "    for user in random_user_list:\n",
    "        items = user_items[user]\n",
    "        if user not in user2id:\n",
    "            user2id[user] = str(user_id)\n",
    "            id2user[str(user_id)] = user\n",
    "            user_id += 1\n",
    "        iids = [] # item id lists\n",
    "        for item in items:\n",
    "            if item not in item2id:\n",
    "                item2id[item] = str(item_id)\n",
    "                id2item[str(item_id)] = item\n",
    "                item_id += 1\n",
    "            iids.append(item2id[item])\n",
    "        uid = user2id[user]\n",
    "        final_data[uid] = iids\n",
    "    data_maps = {\n",
    "        'user2id': user2id,\n",
    "        'item2id': item2id,\n",
    "        'id2user': id2user,\n",
    "        'id2item': id2item\n",
    "    }\n",
    "    return final_data, user_id-1, item_id-1, data_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_name, acronym, data_type='Amazon'):\n",
    "    assert data_type in {'Amazon', 'Yelp'}\n",
    "    rating_score = 0.0  # rating score smaller than this score would be deleted\n",
    "    # user 5-core item 5-core\n",
    "    user_core = 5\n",
    "    item_core = 5\n",
    "    attribute_core = 0\n",
    "\n",
    "    if data_type == 'Yelp':\n",
    "        date_max = '2019-12-31 00:00:00'\n",
    "        date_min = '2019-01-01 00:00:00'\n",
    "        review_data = Yelp(date_min, date_max, rating_score)\n",
    "    else:\n",
    "        review_data = get_reviews(data_name, rating_score=rating_score)\n",
    "\n",
    "    user_items = get_interaction(review_data)\n",
    "    rprint(f'{data_name} Raw data has been processed! Lower than {rating_score} are deleted!')\n",
    "    \n",
    "    # raw_id user: [item1, item2, item3...]\n",
    "    user_items = filter_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    rprint(f'User {user_core}-core complete! Item {item_core}-core complete!')\n",
    "\n",
    "    user_items, user_num, item_num, data_maps = id_map(user_items)\n",
    "    user_count, item_count, _ = check_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    user_count_list = list(user_count.values())\n",
    "    user_avg, user_min, user_max = np.mean(user_count_list), np.min(user_count_list), np.max(user_count_list)\n",
    "    item_count_list = list(item_count.values())\n",
    "    item_avg, item_min, item_max = np.mean(item_count_list), np.min(item_count_list), np.max(item_count_list)\n",
    "    interact_num = np.sum([x for x in user_count_list])\n",
    "    sparsity = (1 - interact_num / (user_num * item_num)) * 100\n",
    "    show_info = f'Total User: {user_num}, Avg User: {user_avg:.4f}, Min Len: {user_min}, Max Len: {user_max}\\n' + \\\n",
    "                f'Total Item: {item_num}, Avg Item: {item_avg:.4f}, Min Inter: {item_min}, Max Inter: {item_max}\\n' + \\\n",
    "                f'Iteraction Num: {interact_num}, Sparsity: {sparsity:.2f}%'\n",
    "    rprint(show_info)\n",
    "\n",
    "\n",
    "    rprint('Begin extracting meta infos...')\n",
    "\n",
    "    if data_type == 'Amazon':\n",
    "        meta_infos = get_metadata(data_name, data_maps)\n",
    "        attribute_num, avg_attribute, datamaps, item2attributes = get_attribute_amazon(meta_infos, data_maps, attribute_core)\n",
    "    else:\n",
    "        meta_infos = Yelp_meta(data_maps)\n",
    "        attribute_num, avg_attribute, datamaps, item2attributes = get_attribute_Yelp(meta_infos, data_maps, attribute_core)\n",
    "\n",
    "    rprint(f'{data_name} & {add_comma(user_num)} & {add_comma(item_num)} & {user_avg:.1f}'\n",
    "          f'& {item_avg:.1f} & {add_comma(interact_num)} & {sparsity:.2f}\\%&{add_comma(attribute_num)} &'\n",
    "          f'{avg_attribute:.1f} \\\\')\n",
    "\n",
    "    # -------------- Save Data ---------------\n",
    "    seq_data_file = os.path.join(DATASET_DIR, acronym, 'sequential_data.txt')\n",
    "    item2attributes_file = os.path.join(DATASET_DIR, acronym, 'item2attributes.json')\n",
    "    datamaps_file = os.path.join(DATASET_DIR, acronym, 'datamaps.json')\n",
    "\n",
    "    with open(seq_data_file, 'w') as out:\n",
    "        for user, items in user_items.items():\n",
    "            out.write(user + ' ' + ' '.join(items) + '\\n')\n",
    "    json_str = json.dumps(item2attributes)\n",
    "    with open(item2attributes_file, 'w') as out:\n",
    "        out.write(json_str)\n",
    "        \n",
    "    json_str = json.dumps(datamaps)\n",
    "    with open(datamaps_file, 'w') as out:\n",
    "        out.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving the K-Core for `Sports_and_Outdoors` Reviews\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving the K-Core for `Sports_and_Outdoors` Reviews\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Reading Ratings<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Reading Ratings\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe24a06d7974797818ae01398ccb27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sports_and_Outdoors.jsonl:   0%|          | 0.00/9.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8ec6307aa47f38139afc15e699add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Reading Ratings<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Reading Ratings\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1d3a79cbcf46aeb34a89209991a6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sports_and_Outdoors.csv:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Filtering Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Filtering Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "category = \"toys\"\n",
    "category = \"beauty\"\n",
    "category = \"sports\"\n",
    "main(DATA_NAME_MAP[category], category, data_type='Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_data(data_name, test_num=99, sample_type='random'):\n",
    "    \"\"\"\n",
    "    sample_type:\n",
    "        random:  sample `test_num` negative items randomly.\n",
    "        pop: sample `test_num` negative items according to item popularity.\n",
    "    \"\"\"\n",
    "\n",
    "    data_file = f'sequential_data.txt'\n",
    "    test_file = f'negative_samples.txt'\n",
    "\n",
    "    item_count = defaultdict(int)\n",
    "    user_items = defaultdict()\n",
    "\n",
    "    lines = open('./{}/'.format(data_name) + data_file).readlines()\n",
    "    for line in lines:\n",
    "        user, items = line.strip().split(' ', 1)\n",
    "        items = items.split(' ')\n",
    "        items = [int(item) for item in items]\n",
    "        user_items[user] = items\n",
    "        for item in items:\n",
    "            item_count[item] += 1\n",
    "\n",
    "    all_item = list(item_count.keys())\n",
    "    count = list(item_count.values())\n",
    "    sum_value = np.sum([x for x in count])\n",
    "    probability = [value / sum_value for value in count]\n",
    "\n",
    "    user_neg_items = defaultdict()\n",
    "\n",
    "    for user, user_seq in user_items.items():\n",
    "        test_samples = []\n",
    "        while len(test_samples) < test_num:\n",
    "            if sample_type == 'random':\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False)\n",
    "            else: # sample_type == 'pop':\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False, p=probability)\n",
    "            sample_ids = [str(item) for item in sample_ids if item not in user_seq and item not in test_samples]\n",
    "            test_samples.extend(sample_ids)\n",
    "        test_samples = test_samples[:test_num]\n",
    "        user_neg_items[user] = test_samples\n",
    "\n",
    "    with open('./{}/'.format(data_name) + test_file, 'w') as out:\n",
    "        for user, samples in user_neg_items.items():\n",
    "            out.write(user+' '+' '.join(samples)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data(short_data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Splits for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['user2id', 'item2id', 'id2user', 'id2item', 'attribute2id', 'id2attribute', 'attributeid2num'])\n"
     ]
    }
   ],
   "source": [
    "datamaps = load_json(os.path.join(DATASET_DIR, short_data_name, \"datamaps.json\"))\n",
    "print(datamaps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296337\n",
      "{'reviewerID': 'AIXZKN4ACSKI', 'asin': '1881509818', 'reviewerName': 'David Briner', 'helpful': [0, 0], 'reviewText': 'This came in on time and I am veru happy with it, I haved used it already and it makes taking out the pins in my glock 32 very easy', 'overall': 5.0, 'summary': 'Woks very good', 'unixReviewTime': 1390694400, 'reviewTime': '01 26, 2014'}\n",
      "296337\n",
      "{'user': 'AIXZKN4ACSKI', 'item': '1881509818', 'rating': 5, 'text': 'Woks very good\\nThis came in on time and I am veru happy with it, I haved used it already and it makes taking out the pins in my glock 32 very easy', 'sentence': [('cam', 'in', 'This came in on time and I am veru happy with it', 1)]}\n",
      "35598\n",
      "18357\n",
      "sparsity:  0.045348045456958995\n"
     ]
    }
   ],
   "source": [
    "review_data = []\n",
    "for review in parse(\"./raw_data/reviews_{}_5.json.gz\".format(full_data_name)):\n",
    "    review_data.append(review)\n",
    "print(len(review_data))\n",
    "print(review_data[0])\n",
    "\n",
    "raw_explanations = load_pickle('./raw_data/reviews_{}.pickle'.format(full_data_name))\n",
    "print(len(raw_explanations))\n",
    "print(raw_explanations[0])\n",
    "\n",
    "print(len(datamaps['user2id']))\n",
    "print(len(datamaps['item2id']))\n",
    "sparsity = 100.0 * len(review_data) / (len(datamaps['user2id']) * len(datamaps['item2id'])) \n",
    "print('sparsity: ', sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296337/296337 [00:00<00:00, 1585898.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_review_indices = []\n",
    "for i in tqdm(range(len(review_data))):\n",
    "    if review_data[i]['reviewerID'] in datamaps['user2id'] and review_data[i]['asin'] in datamaps['item2id']:\n",
    "        valid_review_indices.append(i)\n",
    "print(len(valid_review_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_review_data = []\n",
    "no_sentence = 0\n",
    "for i in range(len(review_data)):\n",
    "    rev_ = review_data[i]\n",
    "    exp_ = raw_explanations[i]\n",
    "    assert rev_['reviewerID'] == exp_['user']\n",
    "    assert rev_['asin'] == exp_['item']\n",
    "    if 'sentence' in exp_:\n",
    "        list_len = len(exp_['sentence'])\n",
    "        selected_idx = random.randint(0, list_len-1)\n",
    "        rev_['explanation'] = exp_['sentence'][selected_idx][2]\n",
    "        rev_['feature'] = exp_['sentence'][selected_idx][0]   # add a random, or list all possible sentences\n",
    "    else:\n",
    "        no_sentence += 1\n",
    "    combined_review_data.append(rev_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'ANKZUDSZFUMNZ',\n",
       " 'asin': '2094869245',\n",
       " 'reviewerName': 'ronald',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': 'it is bright and it has multiple settings for design and speeds. Assembling it to the bike is fast and simple.',\n",
       " 'overall': 5.0,\n",
       " 'summary': 'rear bike light',\n",
       " 'unixReviewTime': 1386374400,\n",
       " 'reviewTime': '12 7, 2013',\n",
       " 'explanation': 'it is bright and it has multiple settings for design and speeds',\n",
       " 'feature': 'settings'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_review_data[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata for Users & Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id2name = {}\n",
    "for i in range(len(combined_review_data)):\n",
    "    user_id = datamaps['user2id'][combined_review_data[i]['reviewerID']]\n",
    "    if 'reviewerName' in combined_review_data[i]:\n",
    "        user_id2name[user_id] = combined_review_data[i]['reviewerName']\n",
    "    else:\n",
    "        user_id2name[user_id] = combined_review_data[i]['reviewerID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(user_id2name, '{}/user_id2name.pkl'.format(short_data_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296337\n"
     ]
    }
   ],
   "source": [
    "population = len(review_data)\n",
    "print(population)\n",
    "data = range(population)\n",
    "\n",
    "user_mention_dict = {}\n",
    "item_mention_dict = {}\n",
    "for i in data:\n",
    "    review_datum = review_data[i]\n",
    "    user_ = review_datum['reviewerID']\n",
    "    item_ = review_datum['asin']\n",
    "    if user_ not in user_mention_dict:\n",
    "        user_mention_dict[user_] = [i]\n",
    "    else:\n",
    "        user_mention_dict[user_].append(i)\n",
    "    if item_ not in item_mention_dict:\n",
    "        item_mention_dict[item_] = [i]\n",
    "    else:\n",
    "        item_mention_dict[item_].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35598/35598 [00:08<00:00, 4379.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18357/18357 [00:08<00:00, 2105.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_indices = []\n",
    "for u in tqdm(user_mention_dict.keys()):\n",
    "    index_cand = user_mention_dict[u]\n",
    "    random_choice = random.randint(0, len(index_cand)-1)\n",
    "    if index_cand[random_choice] not in train_indices:\n",
    "        train_indices.append(index_cand[random_choice])\n",
    "for it in tqdm(item_mention_dict.keys()):\n",
    "    index_cand = item_mention_dict[it]\n",
    "    random_choice = random.randint(0, len(index_cand)-1)\n",
    "    if index_cand[random_choice] not in train_indices:\n",
    "        train_indices.append(index_cand[random_choice])\n",
    "print(len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244637\n"
     ]
    }
   ],
   "source": [
    "remaining_indices = list(set(range(population)).difference(set(train_indices))) # in population but not in train_indices\n",
    "        \n",
    "print(len(remaining_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185370\n",
      "237070\n"
     ]
    }
   ],
   "source": [
    "sub_indices = random.sample(range(len(remaining_indices)), round(population * 0.8) - len(train_indices))\n",
    "print(len(sub_indices))\n",
    "\n",
    "final_train_indices = list(set(np.array(remaining_indices)[sub_indices]).union(set(train_indices)))\n",
    "print(len(final_train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59267\n"
     ]
    }
   ],
   "source": [
    "val_test_indices = list(set(range(population)).difference(set(final_train_indices)))\n",
    "print(len(val_test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29634\n",
      "29633\n",
      "296337\n"
     ]
    }
   ],
   "source": [
    "sub_sub_indices = random.sample(range(len(val_test_indices)), round(population * 0.1))\n",
    "\n",
    "val_indices = list(np.array(val_test_indices)[sub_sub_indices])\n",
    "test_indices = list(set(val_test_indices).difference(set(val_indices)))\n",
    "print(len(val_indices))\n",
    "print(len(test_indices))\n",
    "\n",
    "all_indices = final_train_indices + val_indices + test_indices\n",
    "print(len(set(all_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review_data = []\n",
    "for i in final_train_indices:\n",
    "    train_review_data.append(combined_review_data[i])\n",
    "    \n",
    "val_review_data = []\n",
    "for j in val_indices:\n",
    "    val_review_data.append(combined_review_data[j])\n",
    "    \n",
    "test_review_data = []\n",
    "for k in test_indices:\n",
    "    test_review_data.append(combined_review_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'train': train_review_data,\n",
    "           'val': val_review_data,\n",
    "           'test': test_review_data,\n",
    "           'train_indices': final_train_indices,\n",
    "           'val_indices': val_indices,\n",
    "           'test_indices': test_indices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(outputs, './{}/review_splits.pkl'.format(short_data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'A3L1RYE41LOMOR',\n",
       " 'asin': '7245456313',\n",
       " 'reviewerName': 'Robecology',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': \"&#8230;.and said they're a great value. Son loves working out, this gives him a lot of choices. Good set of workout bands.\",\n",
       " 'overall': 5.0,\n",
       " 'summary': 'Bought as gift; son loves them',\n",
       " 'unixReviewTime': 1388966400,\n",
       " 'reviewTime': '01 6, 2014'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review_data[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp_data = []\n",
    "for i in final_train_indices:\n",
    "    if 'explanation' in combined_review_data[i]:\n",
    "        train_exp_data.append(combined_review_data[i])\n",
    "        \n",
    "val_exp_data = []\n",
    "for j in val_indices:\n",
    "    if 'explanation' in combined_review_data[j]:\n",
    "        val_exp_data.append(combined_review_data[j])\n",
    "\n",
    "test_exp_data = []\n",
    "for k in test_indices:\n",
    "    if 'explanation' in combined_review_data[k]:\n",
    "        test_exp_data.append(combined_review_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'train': train_exp_data,\n",
    "           'val': val_exp_data,\n",
    "           'test': test_exp_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(outputs, './{}/exp_splits.pkl'.format(short_data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'A2NFEGCOY2TO1Q',\n",
       " 'asin': '7245456259',\n",
       " 'reviewerName': 'Adam',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': 'So it worked well for a couple weeks, but during a lunge workout, it snapped on me.  I liked it and thought it was a great product until this happened.  I noticed small rips on the band.  This could have been the issue.',\n",
       " 'overall': 2.0,\n",
       " 'summary': \"resistance was good but quality wasn't\",\n",
       " 'unixReviewTime': 1395964800,\n",
       " 'reviewTime': '03 28, 2014',\n",
       " 'explanation': 'I liked it and thought it was a great product until this happened',\n",
       " 'feature': 'product'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_exp_data[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-balancing Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = load_pickle('./{}/review_splits.pkl'.format(short_data_name))\n",
    "train_review_data = data_splits['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation of Minority Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237070\n",
      "{1.0: 0.030746193107520987, 2.0: 0.03457206732188805, 3.0: 0.08111106424262876, 4.0: 0.218770827181845, 5.0: 0.6347998481461172}\n"
     ]
    }
   ],
   "source": [
    "counts = {float(r): 0 for r in range(1,6)}\n",
    "for record in train_review_data:\n",
    "    counts[record['overall']] += 1\n",
    "T = sum(counts.values())\n",
    "print(T)\n",
    "for k,c in counts.items():\n",
    "    counts[k] = float(counts[k])/T\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = [float(r) for r in range(1,6)]\n",
    "plt.bar(X, [counts[x] for x in X])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 0, -0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1915, 2.2976, 3.1845, 4.6255, 4.8774, 5.5460, 6.0755, 8.4906, 9.4123,\n",
       "        9.9746])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How should each class augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 0.219253806892479, 2.0: 0.21542793267811194, 3.0: 0.16888893575737124, 4.0: 0.031229172818154988, 5.0: 0.0}\n",
      "0.6347998481461171\n",
      "{1.0: 11.233600577502326, 2.0: 9.816119789689461, 3.0: 3.2800788593263976, 4.0: 0.22487141771370195, 5.0: 0.0}\n"
     ]
    }
   ],
   "source": [
    "augment_prob = {r: max(1.0 / (len(counts)-1) - c,0.) for r,c in counts.items()}\n",
    "print(augment_prob)\n",
    "sum_P = sum(augment_prob.values())\n",
    "print(sum_P)\n",
    "all_ratings = [float(r) for r in range(1,6)]\n",
    "augment_multiplier = {r: augment_prob[r] / (sum_P * counts[r]) for r in all_ratings}\n",
    "print(augment_multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Augment with variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: array([0.77760498, 0.155521  , 0.0466563 , 0.0155521 , 0.00466563]), 2.0: array([0.13513514, 0.67567568, 0.13513514, 0.04054054, 0.01351351]), 3.0: array([0.03947368, 0.13157895, 0.65789474, 0.13157895, 0.03947368]), 4.0: array([0.01351351, 0.04054054, 0.13513514, 0.67567568, 0.13513514]), 5.0: array([0.00466563, 0.0155521 , 0.0466563 , 0.155521  , 0.77760498])}\n",
      "{1.0: 77087, 2.0: 75852, 3.0: 58077, 4.0: 20373, 5.0: 5584}\n",
      "236973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dist_to_prob = [0.5,0.1,0.03,0.01,0.003]\n",
    "rating_perturbation_prob = {}\n",
    "for r in all_ratings:\n",
    "    P = np.array([dist_to_prob[int(abs(r - r_2))] for r_2 in all_ratings])\n",
    "    P /= np.sum(P)\n",
    "    rating_perturbation_prob[r] = P\n",
    "print(rating_perturbation_prob)\n",
    "augmented_counts = {r: 0 for r in all_ratings}\n",
    "augmented_records = []\n",
    "for record in train_review_data:\n",
    "    record_rating = record['overall']\n",
    "    M = augment_multiplier[record_rating]\n",
    "    remainder = M % 1\n",
    "    M = int(M)+1 if np.random.random() < remainder else int(M)\n",
    "    sample_amount = np.random.multinomial(M, rating_perturbation_prob[record_rating])\n",
    "    for i,n_sample in enumerate(sample_amount):\n",
    "        for j in range(n_sample):\n",
    "            new_record = {k:v for k,v in record.items()}\n",
    "            new_record['overall'] = float(i+1)\n",
    "            augmented_records.append(new_record)\n",
    "        augmented_counts[float(i+1)] += n_sample\n",
    "print(augmented_counts)\n",
    "print(len(augmented_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474043\n"
     ]
    }
   ],
   "source": [
    "data_splits['train'] = data_splits['train'] + augmented_records\n",
    "print(len(data_splits['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(data_splits, './{}/rating_splits_augmented.pkl'.format(short_data_name)) # for rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. New counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review_data = data_splits['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474043\n",
      "{1.0: 0.1779922918385041, 2.0: 0.1773003714852872, 3.0: 0.16307803300544466, 4.0: 0.15238491022966272, 5.0: 0.32924439344110135}\n"
     ]
    }
   ],
   "source": [
    "counts = {float(r): 0 for r in range(1,6)}\n",
    "for record in train_review_data:\n",
    "    counts[record['overall']] += 1\n",
    "T = sum(counts.values())\n",
    "print(T)\n",
    "for k,c in counts.items():\n",
    "    counts[k] = float(counts[k])/T\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = [float(r) for r in range(1,6)]\n",
    "plt.bar(X, [counts[x] for x in X])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
